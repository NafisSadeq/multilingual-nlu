{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files=os.listdir(\"amazon-massive-dataset-1.0/1.0/data\")\n",
    "input_files.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_utt_annotation(datum):\n",
    "    \n",
    "    utt=datum['annot_utt']\n",
    "    domain=datum['scenario']\n",
    "    intent=datum['intent']\n",
    "    utt=utt.replace(\"[\",\" [ \")\n",
    "    utt=utt.replace(\"]\",\" ] \")\n",
    "    utt=utt.split()\n",
    "    \n",
    "    token_list=[]\n",
    "    tag_list=[]\n",
    "    slot=\"unk\"\n",
    "    flag_in_slot=False\n",
    "    triple=[\"\",\"\",[]]\n",
    "    triple_list=[]\n",
    "    count_triple=0\n",
    "    \n",
    "    for i in range(0,len(utt)):\n",
    "        \n",
    "        #print(utt[i])\n",
    "        \n",
    "        if((utt[i]==\"[\" and utt[i-1]!=\"]\") or utt[i]==\":\" or utt[i]==\"]\"):\n",
    "            \n",
    "            continue\n",
    "        \n",
    "        if(utt[i-1]==\"[\"):\n",
    "            slot=utt[i]\n",
    "            continue\n",
    "            \n",
    "        if(utt[i-1]==\":\"):\n",
    "            token_list.append(utt[i])\n",
    "            tag_list.append(\"B-\"+domain+\"-\"+intent+\"+\"+slot)\n",
    "            flag_in_slot=True\n",
    "            triple[0]=domain+\"-\"+intent\n",
    "            triple[1]=slot\n",
    "            triple[2].append(utt[i])\n",
    "            count_triple+=1\n",
    "        elif(utt[i-1]==\"]\"):\n",
    "            if(utt[i]!=\"[\"):\n",
    "                token_list.append(utt[i])\n",
    "                tag_list.append(\"O\")\n",
    "            flag_in_slot=False\n",
    "            triple[2]=\" \".join(triple[2])\n",
    "            #print(triple)\n",
    "            triple_list.append(triple)\n",
    "            triple=[\"\",\"\",[]]\n",
    "            \n",
    "        elif(flag_in_slot):\n",
    "            token_list.append(utt[i])\n",
    "            tag_list.append(\"I-\"+domain+\"-\"+intent+\"+\"+slot)\n",
    "            flag_in_slot=True\n",
    "            triple[2].append(utt[i])\n",
    "        else:\n",
    "            token_list.append(utt[i])\n",
    "            tag_list.append(\"O\")\n",
    "            \n",
    "            \n",
    "    intent_list=[domain+\"-\"+intent]\n",
    "    \n",
    "    assert len(token_list)==len(tag_list)\n",
    "    assert len(triple_list)==count_triple\n",
    "    \n",
    "    return [token_list,tag_list,intent_list,triple_list,[]]\n",
    "\n",
    "def process_corpus(data):\n",
    "    \n",
    "    processed_data=[]\n",
    "    all_intent_list=[]\n",
    "    all_tag_list=[]\n",
    "\n",
    "    for d in data:\n",
    "        new_d=get_utt_annotation(d)\n",
    "        processed_data.append(new_d)\n",
    "        all_intent_list+=new_d[2]\n",
    "        all_tag_list+=new_d[1]\n",
    "        \n",
    "    intent_vocab=list(set(all_intent_list))\n",
    "    tag_vocab=list(set(all_tag_list))\n",
    "    slot_vocab=[\"-\".join(x.split('-')[1:]) for x in tag_vocab if x!=\"O\"]\n",
    "    slot_vocab=list(set(slot_vocab))\n",
    "\n",
    "    random.shuffle(processed_data)\n",
    "    \n",
    "    train_len=int(len(processed_data)*0.8)\n",
    "    valid_len=(len(processed_data)-train_len)//2\n",
    "    test_len=len(processed_data)-train_len-valid_len\n",
    "\n",
    "    train_data=processed_data[:train_len]\n",
    "    valid_data=processed_data[train_len:train_len+valid_len]\n",
    "    test_data=processed_data[train_len+valid_len:]\n",
    "    \n",
    "    \n",
    "    return train_data,valid_data,test_data,intent_vocab,slot_vocab,tag_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e88b10c502ff43cc91d6ce9118f9b89e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for infile in tqdm(input_files[:6]):\n",
    "    \n",
    "    data=[]\n",
    "\n",
    "    with open(\"amazon-massive-dataset-1.0/1.0/data/\"+infile,'r') as file:\n",
    "        for line in file:\n",
    "            data.append(eval(line.strip()))\n",
    "            \n",
    "    train_data,valid_data,test_data,intent_vocab,slot_vocab,tag_vocab=process_corpus(data)\n",
    "    \n",
    "    outdir=\"corpus/\"+infile.split(\".\")[0]+\"/data/processed_data/\"\n",
    "    \n",
    "    if(not os.path.exists(outdir)):\n",
    "        os.makedirs(outdir)\n",
    "        \n",
    "    with open(outdir+\"train_data.json\",'w') as file:\n",
    "        json.dump(train_data,file,indent=4,ensure_ascii=False)\n",
    "\n",
    "    with open(outdir+\"val_data.json\",'w') as file:\n",
    "        json.dump(train_data,file,indent=4,ensure_ascii=False)\n",
    "\n",
    "    with open(outdir+\"test_data.json\",'w') as file:\n",
    "        json.dump(train_data,file,indent=4,ensure_ascii=False)\n",
    "\n",
    "    with open(outdir+\"intent_vocab.json\",'w') as file:\n",
    "        json.dump(intent_vocab,file,indent=4,ensure_ascii=False)\n",
    "\n",
    "    with open(outdir+\"slot_vocab.json\",'w') as file:\n",
    "        json.dump(slot_vocab,file,indent=4,ensure_ascii=False)\n",
    "\n",
    "    with open(outdir+\"tag_vocab.json\",'w') as file:\n",
    "        json.dump(tag_vocab,file,indent=4,ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
